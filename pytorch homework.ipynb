{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7435d214-bec6-4b90-a2e5-1dc8cb6ab02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sin(x):\n",
    "    return torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9c375a-12b6-47cb-9a73-ac24e76f7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x1 = torch.tensor([0.0], requires_grad=True)  # 设定x1的初始值和梯度追踪\n",
    "x2 = torch.tensor([1.0], requires_grad=True)  # 设定x2的初始值和梯度追踪\n",
    "f = torch.log(x1) + x1 * x2 - torch.sin(x2)   # 定义函数f(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0197602b-bf3d-42b0-9877-75032fffc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()  # 对f进行反向传播，计算梯度\n",
    "gradients_x1 = x1.grad  # 获取x1的梯度\n",
    "gradients_x2 = x2.grad  # 获取x2的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9622e5af-ac91-4c17-9a24-4d6c88c9a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x1, x2) = 6.552027225494385\n",
      "df/dx1 = 3.5\n",
      "d(ln(x1))/dx1 = 0.5\n",
      "d(x1*x2)/dx1 = 3.0\n",
      "df/dx2 = 2.989992618560791\n",
      "d(x1*x2)/dx2 = 2.0\n",
      "d(sin(x2))/dx2 = -0.9899924993515015\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义输入变量x1和x2，并允许梯度计算\n",
    "x1 = torch.tensor([2.0], requires_grad=True)\n",
    "x2 = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# 构建计算图\n",
    "# f(x1, x2) = ln(x1) + x1*x2 - sin(x2)\n",
    "ln_x1 = torch.log(x1)\n",
    "x1_x2 = x1 * x2\n",
    "sin_x2 = torch.sin(x2)\n",
    "f = ln_x1 + x1_x2 - sin_x2\n",
    "\n",
    "# 计算所有节点对x1和x2的导数\n",
    "# 对x1求导\n",
    "df_dx1 = torch.autograd.grad(f, x1, create_graph=True)[0]\n",
    "dln_x1_dx1 = torch.autograd.grad(ln_x1, x1, create_graph=True)[0]\n",
    "dx1_x2_dx1 = torch.autograd.grad(x1_x2, x1, create_graph=True)[0]\n",
    "\n",
    "# 对x2求导\n",
    "df_dx2 = torch.autograd.grad(f, x2, create_graph=True)[0]\n",
    "dx1_x2_dx2 = torch.autograd.grad(x1_x2, x2, create_graph=True)[0]\n",
    "dsin_x2_dx2 = torch.autograd.grad(sin_x2, x2, create_graph=True)[0]\n",
    "\n",
    "# 输出结果\n",
    "print(f\"f(x1, x2) = {f.item()}\")\n",
    "print(f\"df/dx1 = {df_dx1.item()}\")\n",
    "print(f\"d(ln(x1))/dx1 = {dln_x1_dx1.item()}\")\n",
    "print(f\"d(x1*x2)/dx1 = {dx1_x2_dx1.item()}\")\n",
    "print(f\"df/dx2 = {df_dx2.item()}\")\n",
    "print(f\"d(x1*x2)/dx2 = {dx1_x2_dx2.item()}\")\n",
    "print(f\"d(sin(x2))/dx2 = {dsin_x2_dx2.item()}\")\n",
    "\n",
    "# 节点包括x1, x2, ln(x1), x1*x2, sin(x2), f，\n",
    "# 并标注每个节点对x1和x2的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186b386-8cd1-4913-b477-136a199b57a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
